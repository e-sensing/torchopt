<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Advanced Optimizers for Torch • torchopt</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="bootstrap-toc.css">
<script src="bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="Advanced Optimizers for Torch">
<meta property="og:description" content="Optimizers for torch deep learning library. These
    functions include recent results published in the literature and are
    not part of the optimizers offered in torch. Prospective users
    should test these optimizers with their data, since performance
    depends on the specific problem being solved.  The packages includes
    the following optimizers: (a) adabelief by Zhuang et al (2020),
    &lt;arXiv:2010.07468&gt;; (b) adabound by Luo et al.(2019),
    &lt;arXiv:1902.09843&gt;; (c) adamw by Loshchilov &amp; Hutter (2019),
    &lt;arXiv:1711.05101&gt;; (d) madgrad by Defazio and Jelassi (2021),
    &lt;arXiv:2101.11075&gt;; (e) nadam by Dozat (2019),
    &lt;https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf&gt;; (f) qhadam by
    Ma and Yarats(2019), &lt;arXiv:1810.06801&gt;; (g) radam by Liu et al.
    (2019), &lt;arXiv:1908.03265&gt;; (h) swats by Shekar and Sochee (2018),
    &lt;arXiv:1712.07628&gt;; (i) yogi by Zaheer et al.(2019),
    &lt;https:://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization&gt;. ">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    

    <div class="container template-home">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">torchopt</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="reference/index.html">Reference</a>
</li>
<li>
  <a href="news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/e-sensing/torchopt/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><div class="row">
  <div class="contents col-md-9">
<div class="section level1">
<div class="page-header"><h1 id="torchopt">torchopt<a class="anchor" aria-label="anchor" href="#torchopt"></a>
</h1></div>
<!-- badges: start -->
<!-- badges: end --><p>The <code>torchopt</code> package provides R implementation of deep learning optimizers proposed in the literature. It is intended to support the use of the torch package in R.</p>
<div class="section level2">
<h2 id="installation">Installation<a class="anchor" aria-label="anchor" href="#installation"></a>
</h2>
<p>To install the development version of <code>torchopt</code> do as :</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://devtools.r-lib.org/" class="external-link">devtools</a></span><span class="op">)</span>
<span class="fu">install_github</span><span class="op">(</span><span class="st">"e-sensing/torchopt"</span><span class="op">)</span></code></pre></div>
</div>
<div class="section level2">
<h2 id="provided-optimizers">Provided optimizers<a class="anchor" aria-label="anchor" href="#provided-optimizers"></a>
</h2>
<p><code>torchopt</code> package provides the following R implementations of torch optimizers:</p>
<ul>
<li><p><code><a href="reference/optim_adamw.html">optim_adamw()</a></code>: AdamW optimizer proposed by Loshchilov &amp; Hutter (2019). Converted from the <code>pytorch</code> code developed by Collin Donahue-Oponski available at <a href="https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3" class="external-link uri">https://gist.github.com/colllin/0b146b154c4351f9a40f741a28bff1e3</a></p></li>
<li><p><code><a href="reference/optim_adabelief.html">optim_adabelief()</a></code>: Adabelief optimizer proposed by Zhuang et al (2020). Converted from the authors’ PyTorch code: <a href="https://github.com/juntang-zhuang/Adabelief-Optimizer" class="external-link uri">https://github.com/juntang-zhuang/Adabelief-Optimizer</a>.</p></li>
<li><p><code><a href="reference/optim_adabound.html">optim_adabound()</a></code>: Adabound optimizer proposed by Luo et al.(2019). Converted from the authors’ PyTorch code: <a href="https://github.com/Luolc/AdaBound" class="external-link uri">https://github.com/Luolc/AdaBound</a>.</p></li>
<li><p><code><a href="reference/optim_adahessian.html">optim_adahessian()</a></code>: Adahessian optimizer proposed by Luo et al.(2019). Converted from the authors’ PyTorch code: <a href="https://github.com/Luolc/AdaBound" class="external-link uri">https://github.com/Luolc/AdaBound</a>.</p></li>
<li><p><code><a href="reference/optim_madgrad.html">optim_madgrad()</a></code>: Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization (MADGRAD) optimizer proposed by Defazio &amp; Jelassi (2021). The function is imported from <a href="https://CRAN.R-project.org/package=madgrad" class="external-link">madgrad</a> package and the source code is available at <a href="https://github.com/mlverse/madgrad" class="external-link uri">https://github.com/mlverse/madgrad</a></p></li>
<li><p><code><a href="reference/optim_nadam.html">optim_nadam()</a></code>: Incorporation of Nesterov Momentum into Adam proposed by Dozat (2016). Converted from the PyTorch site <a href="https://github.com/pytorch/pytorch" class="external-link uri">https://github.com/pytorch/pytorch</a>.</p></li>
<li><p><code><a href="reference/optim_qhadam.html">optim_qhadam()</a></code>: Quasi-hyperbolic version of Adam proposed by Ma and Yarats(2019). Converted from the code developed by Meta AI: <a href="https://github.com/facebookresearch/qhoptim" class="external-link uri">https://github.com/facebookresearch/qhoptim</a>.</p></li>
<li><p><code><a href="reference/optim_radam.html">optim_radam()</a></code>: Rectified verison of Adam proposed by Liu et al. (2019). Converted from the PyTorch code <a href="https://github.com/pytorch/pytorch" class="external-link uri">https://github.com/pytorch/pytorch</a>.</p></li>
<li><p><code><a href="reference/optim_swats.html">optim_swats()</a></code>: Optimizer that switches from Adam to SGD proposed by Keskar and Socher(2018). Converted from the <code>pytorch</code> code developed by Patrik Purgai: <a href="https://github.com/Mrpatekful/swats" class="external-link uri">https://github.com/Mrpatekful/swats</a></p></li>
<li><p><code><a href="reference/optim_yogi.html">optim_yogi()</a></code>: Yogi optimizer proposed by Zaheer et al.(2019). Converted from the <code>pytorch</code> code developed by Nikolay Novik: <a href="https://github.com/jettify/pytorch-optimizer" class="external-link uri">https://github.com/jettify/pytorch-optimizer</a></p></li>
</ul>
</div>
<div class="section level2">
<h2 id="optimization-test-functions">Optimization test functions<a class="anchor" aria-label="anchor" href="#optimization-test-functions"></a>
</h2>
<p>You can also test optimizers using optimization <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization" class="external-link">test functions</a> provided by <code>torchopt</code> including <code>"ackley"</code>, <code>"beale"</code>, <code>"booth"</code>, <code>"bukin_n6"</code>, <code>"easom"</code>, <code>"goldstein_price"</code>, <code>"himmelblau"</code>, <code>"levi_n13"</code>, <code>"matyas"</code>, <code>"rastrigin"</code>, <code>"rosenbrock"</code>, <code>"sphere"</code>. Optimization functions are useful to evaluate characteristics of optimization algorithms, such as convergence rate, precision, robustness, and performance. These functions give an idea about the different situations that optimization algorithms can face.</p>
<p>In what follows, we perform tests using <code>"beale"</code> test function. To visualize an animated GIF, we set <code>plot_each_step=TRUE</code> and capture each step frame using <a href="https://CRAN.R-project.org/package=gifski" class="external-link">gifski</a> package.</p>
<div class="section level3">
<h3 id="optim_adamw">
<code>optim_adamw()</code>:<a class="anchor" aria-label="anchor" href="#optim_adamw"></a>
</h3>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># test optim adamw</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">12345</span><span class="op">)</span>
<span class="fu">torchopt</span><span class="fu">::</span><span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="fu">torchopt</span><span class="fu">::</span><span class="va"><a href="reference/optim_adamw.html">optim_adamw</a></span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">500</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_adamw-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_adabelief">
<code>optim_adabelief()</code>:<a class="anchor" aria-label="anchor" href="#optim_adabelief"></a>
</h3>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">42</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_adabelief</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">400</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_adabelief-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_adabound">
<code>optim_adabound()</code>:<a class="anchor" aria-label="anchor" href="#optim_adabound"></a>
</h3>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># set manual seed</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">22</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_adabound</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">400</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_adabound-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_adahessian">
<code>optim_adahessian()</code>:<a class="anchor" aria-label="anchor" href="#optim_adahessian"></a>
</h3>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># set manual seed</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">290356</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_adahessian</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.2</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">500</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_adahessian-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_madgrad">
<code>optim_madgrad()</code>:<a class="anchor" aria-label="anchor" href="#optim_madgrad"></a>
</h3>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">256</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_madgrad</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.05</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">400</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_madgrad-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_nadam">
<code>optim_nadam()</code>:<a class="anchor" aria-label="anchor" href="#optim_nadam"></a>
</h3>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">2903</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_nadam</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.5</span>, weight_decay <span class="op">=</span> <span class="fl">0</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">500</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_nadam-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_qhadam">
<code>optim_qhadam()</code>:<a class="anchor" aria-label="anchor" href="#optim_qhadam"></a>
</h3>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1024</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_qhadam</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">500</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_qhadam-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_radam">
<code>optim_radam()</code>:<a class="anchor" aria-label="anchor" href="#optim_radam"></a>
</h3>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1024</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_radam</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">1.0</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">500</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_radam-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_swats">
<code>optim_swats()</code>:<a class="anchor" aria-label="anchor" href="#optim_swats"></a>
</h3>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">234</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_swats</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">500</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_swats-.gif" width="50%" height="50%"></p>
</div>
<div class="section level3">
<h3 id="optim_yogi">
<code>optim_yogi()</code>:<a class="anchor" aria-label="anchor" href="#optim_yogi"></a>
</h3>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># set manual seed</span>
<span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">66</span><span class="op">)</span>
<span class="fu"><a href="reference/test_optim.html">test_optim</a></span><span class="op">(</span>
    optim <span class="op">=</span> <span class="va">optim_yogi</span>,
    opt_hparams <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span>,
    steps <span class="op">=</span> <span class="fl">500</span>,
    test_fn <span class="op">=</span> <span class="st">"beale"</span>,
    plot_each_step <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
<p><img src="reference/figures/README-test_yogi-.gif" width="50%" height="50%"></p>
</div>
</div>
<div class="section level2">
<h2 id="acknowledgements">Acknowledgements<a class="anchor" aria-label="anchor" href="#acknowledgements"></a>
</h2>
<p>We are thankful to Collin Donahue-Oponski <a href="https://github.com/colllin" class="external-link uri">https://github.com/colllin</a>, Amir Gholami <a href="https://github.com/amirgholami" class="external-link uri">https://github.com/amirgholami</a>, Liangchen Luo <a href="https://github.com/Luolc" class="external-link uri">https://github.com/Luolc</a>, Liyuan Liu <a href="https://github.com/LiyuanLucasLiu" class="external-link uri">https://github.com/LiyuanLucasLiu</a>, Nikolay Novik <a href="https://github.com/jettify" class="external-link uri">https://github.com/jettify</a>, Patrik Purgai <a href="https://github.com/Mrpatekful" class="external-link uri">https://github.com/Mrpatekful</a> Juntang Zhuang <a href="https://github.com/juntang-zhuang" class="external-link uri">https://github.com/juntang-zhuang</a> and the PyTorch team <a href="https://github.com/pytorch/pytorch" class="external-link uri">https://github.com/pytorch/pytorch</a> for providing pytorch code for the optimizers implemented in this package. We also thank Daniel Falbel <a href="https://github.com/dfalbel" class="external-link uri">https://github.com/dfalbel</a> for providing support for the R version of PyTorch.</p>
</div>
<div class="section level2">
<h2 id="code-of-conduct">Code of Conduct<a class="anchor" aria-label="anchor" href="#code-of-conduct"></a>
</h2>
<p>The torchopt project is released with a <a href="https://contributor-covenant.org/version/2/0/CODE_OF_CONDUCT.html" class="external-link">Contributor Code of Conduct</a>. By contributing to this project, you agree to abide by its terms.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ul>
<li><p>ADABELIEF: Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek, Xenophon Papademetris, James S. Duncan. “Adabelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients”, 34th Conference on Neural Information Processing Systems (NeurIPS 2020), <a href="https://arxiv.org/abs/2010.07468" class="external-link uri">https://arxiv.org/abs/2010.07468</a>.</p></li>
<li><p>ADABOUND: Liangchen Luo, Yuanhao Xiong, Yan Liu, Xu Sun, “Adaptive Gradient Methods with Dynamic Bound of Learning Rate”, International Conference on Learning Representations (ICLR), 2019. <a href="https://doi.org/10.48550/arXiv.1902.09843" class="external-link uri">https://doi.org/10.48550/arXiv.1902.09843</a>.</p></li>
<li><p>ADAHESSIAN: Zhewei Yao, Amir Gholami, Sheng Shen, Mustafa Mustafa, Kurt Keutzer, Michael W. Mahoney. “Adahessian: An Adaptive Second Order Optimizer for Machine Learning”, AAAI Conference on Artificial Intelligence, 35(12), 10665-10673, 2021. <a href="https://arxiv.org/abs/2006.00719" class="external-link uri">https://arxiv.org/abs/2006.00719</a>.</p></li>
<li><p>ADAMW: Ilya Loshchilov, Frank Hutter, “Decoupled Weight Decay Regularization”, International Conference on Learning Representations (ICLR) 2019. <a href="https://doi.org/10.48550/arXiv.1711.05101" class="external-link uri">https://doi.org/10.48550/arXiv.1711.05101</a>.</p></li>
<li><p>MADGRAD: Aaron Defazio, Samy Jelassi, “Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization”, arXiv preprint arXiv:2101.11075, 2021. <a href="https://doi.org/10.48550/arXiv.2101.11075" class="external-link uri">https://doi.org/10.48550/arXiv.2101.11075</a></p></li>
<li><p>NADAM: Timothy Dazat, “Incorporating Nesterov Momentum into Adam”, International Conference on Learning Representations (ICLR), 2019. <a href="https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf" class="external-link uri">https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf</a></p></li>
<li><p>QHADAM: Jerry Ma, Denis Yarats, “Quasi-hyperbolic momentum and Adam for deep learning”. <a href="https://arxiv.org/abs/1810.06801" class="external-link uri">https://arxiv.org/abs/1810.06801</a></p></li>
<li><p>RADAM: Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han, “On the Variance of the Adaptive Learning Rate and Beyond”, International Conference on Learning Representations (ICLR) 2020. <a href="https://arxiv.org/abs/1908.03265" class="external-link uri">https://arxiv.org/abs/1908.03265</a>.</p></li>
<li><p>SWATS: Nitish Keskar, Richard Socher, “Improving Generalization Performance by Switching from Adam to SGD”. International Conference on Learning Representations (ICLR), 2018. <a href="https://arxiv.org/abs/1712.07628" class="external-link uri">https://arxiv.org/abs/1712.07628</a>.</p></li>
<li><p>YOGI: Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar, “Adaptive Methods for Nonconvex Optimization”, Advances in Neural Information Processing Systems 31 (NeurIPS 2018). <a href="https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization" class="external-link uri">https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization</a></p></li>
</ul>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <div class="links">
<h2 data-toc-skip>Links</h2>
<ul class="list-unstyled">
<li><a href="https://cloud.r-project.org/package=torchopt" class="external-link">View on CRAN</a></li>
<li><a href="https://github.com/e-sensing/torchopt/" class="external-link">Browse source code</a></li>
</ul>
</div>

<div class="license">
<h2 data-toc-skip>License</h2>
<ul class="list-unstyled">
<li>Apache License (&gt;= 2)</li>
</ul>
</div>

<div class="community">
<h2 data-toc-skip>Community</h2>
<ul class="list-unstyled">
<li><a href="CODE_OF_CONDUCT.html">Code of conduct</a></li>
</ul>
</div>

<div class="citation">
<h2 data-toc-skip>Citation</h2>
<ul class="list-unstyled">
<li><a href="authors.html#citation">Citing torchopt</a></li>
</ul>
</div>

<div class="developers">
<h2 data-toc-skip>Developers</h2>
<ul class="list-unstyled">
<li>Gilberto Camara <br><small class="roles"> Author, maintainer </small>  </li>
<li>Rolf Simoes <br><small class="roles"> Author </small>  </li>
<li>Daniel Falbel <br><small class="roles"> Author </small>  </li>
<li>Felipe Souza <br><small class="roles"> Author </small>  </li>
<li>Alber Sanchez <br><small class="roles"> Author </small>  </li>
</ul>
</div>

<div class="dev-status">
<h2 data-toc-skip>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://github.com/e-sensing/torchopt/actions" class="external-link"><img src="https://github.com/e-sensing/torchopt/workflows/R-CMD-check/badge.svg" alt="R-CMD-check"></a></li>
<li><a href="https://cran.r-project.org/package=torchopt" class="external-link"><img src="https://www.r-pkg.org/badges/version/torchopt" alt="CRAN status"></a></li>
<li><a href="https://lifecycle.r-lib.org/articles/stages.html" class="external-link"><img src="https://img.shields.io/badge/lifecycle-experimental-yellow.svg" alt="Software Life Cycle"></a></li>
<li><a href="https://www.apache.org/licenses/LICENSE-2.0" class="external-link"><img src="https://img.shields.io/badge/license-Apache%202-2--green" alt="Software License"></a></li>
</ul>
</div>

  </div>
</div>


      <footer><div class="copyright">
  <p></p>
<p>Developed by Gilberto Camara, Rolf Simoes, Daniel Falbel, Felipe Souza, Alber Sanchez.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.3.</p>
</div>

      </footer>
</div>

  


  

  </body>
</html>
