<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Yogi optimizer — optim_yogi • torchopt</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Yogi optimizer — optim_yogi"><meta property="og:description" content="R implementation of the Yogi optimizer proposed
by Zaheer et al.(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/yogi.py.
Thanks to Nikolay Novik for providing the pytorch code.
The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.
From the abstract by the paper by Zaheer et al.(2019):
Adaptive gradient methods that rely on scaling gradients
down by the square root of exponential moving averages
of past squared gradients, such RMSProp, Adam, Adadelta have
found wide application in optimizing the nonconvex problems
that arise in deep learning. However, it has been recently
demonstrated that such methods can fail to converge even
in simple convex optimization settings.
Yogi is a new adaptive optimization algorithm,
which controls the increase in effective learning rate,
leading to even better performance with similar theoretical
guarantees on convergence. Extensive experiments show that
Yogi with very little hyperparameter tuning outperforms
methods such as Adam in several challenging machine learning tasks."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">
    

    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">torchopt</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/e-sensing/torchopt/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Yogi optimizer</h1>
    <small class="dont-index">Source: <a href="https://github.com/e-sensing/torchopt/blob/HEAD/R/yogi.R" class="external-link"><code>R/yogi.R</code></a></small>
    <div class="hidden name"><code>optim_yogi.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>R implementation of the Yogi optimizer proposed
by Zaheer et al.(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/yogi.py.
Thanks to Nikolay Novik for providing the pytorch code.</p>
<p>The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.</p>
<p>From the abstract by the paper by Zaheer et al.(2019):
Adaptive gradient methods that rely on scaling gradients
down by the square root of exponential moving averages
of past squared gradients, such RMSProp, Adam, Adadelta have
found wide application in optimizing the nonconvex problems
that arise in deep learning. However, it has been recently
demonstrated that such methods can fail to converge even
in simple convex optimization settings.
Yogi is a new adaptive optimization algorithm,
which controls the increase in effective learning rate,
leading to even better performance with similar theoretical
guarantees on convergence. Extensive experiments show that
Yogi with very little hyperparameter tuning outperforms
methods such as Adam in several challenging machine learning tasks.</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="fu">optim_yogi</span><span class="op">(</span>
  <span class="va">params</span>,
  lr <span class="op">=</span> <span class="fl">0.01</span>,
  betas <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">0.999</span><span class="op">)</span>,
  eps <span class="op">=</span> <span class="fl">0.001</span>,
  initial_accumulator <span class="op">=</span> <span class="fl">1e-06</span>,
  weight_decay <span class="op">=</span> <span class="fl">0</span>
<span class="op">)</span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>
    <dl><dt>params</dt>
<dd><p>List of parameters to optimize.</p></dd>
<dt>lr</dt>
<dd><p>Learning rate (default: 1e-3)</p></dd>
<dt>betas</dt>
<dd><p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p></dd>
<dt>eps</dt>
<dd><p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p></dd>
<dt>initial_accumulator</dt>
<dd><p>Initial values for first and
second moments.</p></dd>
<dt>weight_decay</dt>
<dd><p>Weight decay (L2 penalty) (default: 0)</p></dd>
</dl></div>
    <div id="value">
    <h2>Value</h2>
    <p>A torch optimizer object implementing the <code>step</code> method.</p>
    </div>
    <div id="references">
    <h2>References</h2>
    <p>Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, Sanjiv Kumar,
"Adaptive Methods for Nonconvex Optimization",
Advances in Neural Information Processing Systems 31 (NeurIPS 2018).
https://papers.nips.cc/paper/8186-adaptive-methods-for-nonconvex-optimization</p>
    </div>
    <div id="author">
    <h2>Author</h2>
    <p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a></p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a></p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a></p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a></p>
    </div>

    <div id="ref-examples">
    <h2>Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span class="kw">if</span> <span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in"><span class="co"># function to demonstrate optimization</span></span>
<span class="r-in"><span class="va">beale</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in">    <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="op">(</span><span class="fl">1.5</span> <span class="op">-</span> <span class="va">x</span> <span class="op">+</span> <span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="fl">2.25</span> <span class="op">-</span> <span class="va">x</span> <span class="op">-</span> <span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="op">(</span><span class="fl">2.625</span> <span class="op">-</span> <span class="va">x</span> <span class="op">+</span> <span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">^</span><span class="fl">3</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span class="r-in"> <span class="op">}</span></span>
<span class="r-in"><span class="co"># define optimizer</span></span>
<span class="r-in"><span class="va">optim</span> <span class="op">&lt;-</span> <span class="fu">torchopt</span><span class="fu">::</span><span class="va">optim_yogi</span></span>
<span class="r-in"><span class="co"># define hyperparams</span></span>
<span class="r-in"><span class="va">opt_hparams</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.01</span><span class="op">)</span></span>
<span class="r-in"></span>
<span class="r-in"><span class="co"># starting point</span></span>
<span class="r-in"><span class="va">x0</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span class="r-in"><span class="va">y0</span> <span class="op">&lt;-</span> <span class="fl">3</span></span>
<span class="r-in"><span class="co"># create tensor</span></span>
<span class="r-in"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_tensor.html" class="external-link">torch_tensor</a></span><span class="op">(</span><span class="va">x0</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span class="r-in"><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_tensor.html" class="external-link">torch_tensor</a></span><span class="op">(</span><span class="va">y0</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span class="r-in"><span class="co"># instantiate optimizer</span></span>
<span class="r-in"><span class="va">optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">optim</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>params <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span>, <span class="va">opt_hparams</span><span class="op">)</span><span class="op">)</span></span>
<span class="r-in"><span class="co"># run optimizer</span></span>
<span class="r-in"><span class="va">steps</span> <span class="op">&lt;-</span> <span class="fl">400</span></span>
<span class="r-in"><span class="va">x_steps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span></span>
<span class="r-in"><span class="va">y_steps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span></span>
<span class="r-in"><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in">    <span class="va">x_steps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span class="r-in">    <span class="va">y_steps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span class="r-in">    <span class="va">optim</span><span class="op">$</span><span class="fu">zero_grad</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in">    <span class="va">z</span> <span class="op">&lt;-</span> <span class="fu">beale</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span class="r-in">    <span class="va">z</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in">    <span class="va">optim</span><span class="op">$</span><span class="fu">step</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in"><span class="op">}</span></span>
<span class="r-in"><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"starting value = "</span>, <span class="fu">beale</span><span class="op">(</span><span class="va">x0</span>, <span class="va">y0</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span class="r-in"><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">"final value = "</span>, <span class="fu">beale</span><span class="op">(</span><span class="va">x_steps</span><span class="op">[</span><span class="va">steps</span><span class="op">]</span>, <span class="va">y_steps</span><span class="op">[</span><span class="va">steps</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span class="r-in"><span class="op">}</span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "starting value = 8.89928091539177"</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> [1] "final value = -1.53110110102102"</span>
</code></pre></div>
    </div>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Gilberto Camara, Rolf Simoes, Daniel Falbel, Felipe Souza, Alber Sanchez.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.3.</p>
</div>

      </footer></div>

  


  

  </body></html>

