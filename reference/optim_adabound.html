<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Adabound optimizer — optim_adabound • torchopt</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Adabound optimizer — optim_adabound"><meta property="og:description" content="R implementation of the AdaBound optimizer proposed
by Luo et al.(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/yogi.py.
Thanks to Nikolay Novik for providing the pytorch code.
The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.
AdaBound is a variant of the Adam stochastic optimizer which is
designed to be more robust to extreme learning rates.
Dynamic bounds are employed on learning rates,
where the lower and upper bound are initialized as zero and
infinity respectively, and they both smoothly converge to a
constant final step size. AdaBound can be regarded as an adaptive
method at the beginning of training, and thereafter it gradually and
smoothly transforms to SGD (or with momentum) as the time step increases."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">
    

    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">torchopt</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/e-sensing/torchopt/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Adabound optimizer</h1>
    <small class="dont-index">Source: <a href="https://github.com/e-sensing/torchopt/blob/HEAD/R/adabound.R" class="external-link"><code>R/adabound.R</code></a></small>
    <div class="hidden name"><code>optim_adabound.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>R implementation of the AdaBound optimizer proposed
by Luo et al.(2019). We used the implementation available at
https://github.com/jettify/pytorch-optimizer/blob/master/torch_optimizer/yogi.py.
Thanks to Nikolay Novik for providing the pytorch code.</p>
<p>The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.</p>
<p>AdaBound is a variant of the Adam stochastic optimizer which is
designed to be more robust to extreme learning rates.
Dynamic bounds are employed on learning rates,
where the lower and upper bound are initialized as zero and
infinity respectively, and they both smoothly converge to a
constant final step size. AdaBound can be regarded as an adaptive
method at the beginning of training, and thereafter it gradually and
smoothly transforms to SGD (or with momentum) as the time step increases.</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="fu">optim_adabound</span><span class="op">(</span>
  <span class="va">params</span>,
  lr <span class="op">=</span> <span class="fl">0.001</span>,
  betas <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">0.999</span><span class="op">)</span>,
  final_lr <span class="op">=</span> <span class="fl">0.1</span>,
  gamma <span class="op">=</span> <span class="fl">0.001</span>,
  eps <span class="op">=</span> <span class="fl">1e-08</span>,
  weight_decay <span class="op">=</span> <span class="fl">0</span>
<span class="op">)</span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>
    <dl><dt>params</dt>
<dd><p>List of parameters to optimize.</p></dd>
<dt>lr</dt>
<dd><p>Learning rate (default: 1e-3)</p></dd>
<dt>betas</dt>
<dd><p>Coefficients computing running averages of gradient
and its square (default: (0.9, 0.999))</p></dd>
<dt>final_lr</dt>
<dd><p>Final (SGD) learning rate (default: 0.1)</p></dd>
<dt>gamma</dt>
<dd><p>Convergence speed of the bound functions
(default: 1e-3)</p></dd>
<dt>eps</dt>
<dd><p>Term added to the denominator to improve numerical
stability (default: 1e-8)</p></dd>
<dt>weight_decay</dt>
<dd><p>Weight decay (L2 penalty) (default: 0)</p></dd>
</dl></div>
    <div id="value">
    <h2>Value</h2>
    <p>A torch optimizer object implementing the <code>step</code> method.</p>
    </div>
    <div id="references">
    <h2>References</h2>
    <p>Liangchen Luo, Yuanhao Xiong, Yan Liu, Xu Sun,
"Adaptive Gradient Methods with Dynamic Bound of Learning Rate",
International Conference on Learning Representations (ICLR), 2019.
https://arxiv.org/abs/1902.09843</p>
    </div>
    <div id="author">
    <h2>Author</h2>
    <p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a></p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a></p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a></p>
<p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a></p>
    </div>

    <div id="ref-examples">
    <h2>Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in"><span class="kw">if</span> <span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs" class="external-link">torch</a></span><span class="op">)</span></span>
<span class="r-in"><span class="co"># define test function</span></span>
<span class="r-in"><span class="va">test_matyas</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fl">0.26</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">y</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="fl">0.48</span> <span class="op">*</span> <span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span><span class="op">}</span></span>
<span class="r-in"><span class="co"># define starting point</span></span>
<span class="r-in"><span class="va">x0</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">5</span></span>
<span class="r-in"><span class="va">y0</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">5</span></span>
<span class="r-in"><span class="co"># create tensor</span></span>
<span class="r-in"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_tensor.html" class="external-link">torch_tensor</a></span><span class="op">(</span><span class="va">x0</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span class="r-in"><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_tensor.html" class="external-link">torch_tensor</a></span><span class="op">(</span><span class="va">y0</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span class="r-in"><span class="co"># define optimizer</span></span>
<span class="r-in"><span class="va">optim</span> <span class="op">&lt;-</span> <span class="va">optim_adabound</span></span>
<span class="r-in"><span class="va">opt_hparams</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span class="r-in"><span class="va">optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">optim</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>params <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span>, <span class="va">opt_hparams</span><span class="op">)</span><span class="op">)</span></span>
<span class="r-in"><span class="co"># run optimizer</span></span>
<span class="r-in"><span class="va">steps</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span class="r-in"><span class="va">x_steps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span></span>
<span class="r-in"><span class="va">y_steps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span></span>
<span class="r-in"><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in">     <span class="va">x_steps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">y_steps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">optim</span><span class="op">$</span><span class="fu">zero_grad</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">z</span> <span class="op">&lt;-</span> <span class="fu">test_matyas</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">z</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">optim</span><span class="op">$</span><span class="fu">step</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in"><span class="op">}</span></span>
<span class="r-in"><span class="op">}</span></span>
<span class="r-in"><span class="op">}</span></span>
</code></pre></div>
    </div>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Gilberto Camara, Rolf Simoes, Daniel Falbel, Felipe Souza, Alber Sanchez.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.2.</p>
</div>

      </footer></div>

  


  

  </body></html>

