<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Adabelief optimizer — optim_adabelief • torchopt</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Adabelief optimizer — optim_adabelief"><meta property="og:description" content='R implementation of the adabelief optimizer proposed
by Zhuang et al (2020). We used the pytorch implementation
developed by the authors which is available at
https://github.com/jettify/pytorch-optimizer.
Thanks to Nikolay Novik of his work on python optimizers.
The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.
From the abstract by the paper by Zhuang et al (2021):
We propose Adabelief to simultaneously achieve three goals:
fast convergence as in adaptive methods, good generalization as in SGD,
and training stability. The intuition for AdaBelief is to adapt
the stepsize according to the "belief" in the current gradient direction.
Viewing the exponential moving average of the noisy gradient
as the prediction of the gradient at the next time step,
if the observed gradient greatly deviates from the prediction,
we distrust the current observation and take a small step;
if the observed gradient is close to the prediction,
we trust it and take a large step.'><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">
    

    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">torchopt</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/e-sensing/torchopt/" class="external-link">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Adabelief optimizer</h1>
    <small class="dont-index">Source: <a href="https://github.com/e-sensing/torchopt/blob/HEAD/R/adabelief.R" class="external-link"><code>R/adabelief.R</code></a></small>
    <div class="hidden name"><code>optim_adabelief.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>R implementation of the adabelief optimizer proposed
by Zhuang et al (2020). We used the pytorch implementation
developed by the authors which is available at
https://github.com/jettify/pytorch-optimizer.
Thanks to Nikolay Novik of his work on python optimizers.</p>
<p>The original implementation is licensed using the Apache-2.0 software license.
This implementation is also licensed using Apache-2.0 license.</p>
<p>From the abstract by the paper by Zhuang et al (2021):
We propose Adabelief to simultaneously achieve three goals:
fast convergence as in adaptive methods, good generalization as in SGD,
and training stability. The intuition for AdaBelief is to adapt
the stepsize according to the "belief" in the current gradient direction.
Viewing the exponential moving average of the noisy gradient
as the prediction of the gradient at the next time step,
if the observed gradient greatly deviates from the prediction,
we distrust the current observation and take a small step;
if the observed gradient is close to the prediction,
we trust it and take a large step.</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="fu">optim_adabelief</span><span class="op">(</span>
  <span class="va">params</span>,
  lr <span class="op">=</span> <span class="fl">0.001</span>,
  betas <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.9</span>, <span class="fl">0.999</span><span class="op">)</span>,
  eps <span class="op">=</span> <span class="fl">1e-08</span>,
  weight_decay <span class="op">=</span> <span class="fl">1e-06</span>,
  weight_decouple <span class="op">=</span> <span class="cn">TRUE</span>,
  fixed_decay <span class="op">=</span> <span class="cn">FALSE</span>,
  rectify <span class="op">=</span> <span class="cn">TRUE</span>
<span class="op">)</span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>
    <dl><dt>params</dt>
<dd><p>List of parameters to optimize.</p></dd>
<dt>lr</dt>
<dd><p>Learning rate (default: 1e-3)</p></dd>
<dt>betas</dt>
<dd><p>Coefficients for computing running averages
of gradient and its square (default: (0.9, 0.999))</p></dd>
<dt>eps</dt>
<dd><p>Term added to the denominator to improve numerical
stability (default: 1e-16)</p></dd>
<dt>weight_decay</dt>
<dd><p>Weight decay (L2 penalty) (default: 0)</p></dd>
<dt>weight_decouple</dt>
<dd><p>Use decoupled weight decay as is done in AdamW?</p></dd>
<dt>fixed_decay</dt>
<dd><p>This is used when weight_decouple is set as True.
When fixed_decay == True, weight decay is
W_new = W_old - W_old * decay.
When fixed_decay == False, the weight decay is
W_new = W_old - W_old * decay * learning_rate.
In this case, weight decay decreases with learning rate.</p></dd>
<dt>rectify</dt>
<dd><p>Perform the rectified update similar to RAdam?</p></dd>
</dl></div>
    <div id="value">
    <h2>Value</h2>
    <p>A torch optimizer object implementing the <code>step</code> method.</p>
    </div>
    <div id="references">
    <h2>References</h2>
    <p>Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda,
Nicha Dvornek, Xenophon Papademetris, James S. Duncan.
"Adabelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients",
34th Conference on Neural Information Processing Systems (NeurIPS 2020),
Vancouver, Canada.
https://arxiv.org/abs/2010.07468</p>
    </div>
    <div id="author">
    <h2>Author</h2>
    <p>Gilberto Camara, <a href="mailto:gilberto.camara@inpe.br">gilberto.camara@inpe.br</a></p>
<p>Rolf Simoes, <a href="mailto:rolf.simoes@inpe.br">rolf.simoes@inpe.br</a></p>
<p>Felipe Souza, <a href="mailto:lipecaso@gmail.com">lipecaso@gmail.com</a></p>
<p>Alber Sanchez, <a href="mailto:alber.ipia@inpe.br">alber.ipia@inpe.br</a></p>
    </div>

    <div id="ref-examples">
    <h2>Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span class="kw">if</span> <span class="op">(</span><span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_is_installed.html" class="external-link">torch_is_installed</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in"><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://torch.mlverse.org/docs" class="external-link">torch</a></span><span class="op">)</span></span>
<span class="r-in"><span class="co"># define test function</span></span>
<span class="r-in"><span class="va">test_matyas</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span> <span class="op">{</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="fl">0.26</span> <span class="op">*</span> <span class="op">(</span><span class="va">x</span><span class="op">^</span><span class="fl">2</span> <span class="op">+</span> <span class="va">y</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span> <span class="op">-</span> <span class="fl">0.48</span> <span class="op">*</span> <span class="va">x</span> <span class="op">*</span> <span class="va">y</span><span class="op">)</span><span class="op">}</span></span>
<span class="r-in"><span class="co"># define starting point</span></span>
<span class="r-in"><span class="va">x0</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">5</span></span>
<span class="r-in"><span class="va">y0</span> <span class="op">&lt;-</span> <span class="op">-</span><span class="fl">5</span></span>
<span class="r-in"><span class="co"># create tensor</span></span>
<span class="r-in"><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_tensor.html" class="external-link">torch_tensor</a></span><span class="op">(</span><span class="va">x0</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span class="r-in"><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu">torch</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/torch/man/torch_tensor.html" class="external-link">torch_tensor</a></span><span class="op">(</span><span class="va">y0</span>, requires_grad <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span class="r-in"><span class="co"># define optimizer</span></span>
<span class="r-in"><span class="va">optim</span> <span class="op">&lt;-</span> <span class="va">optim_adabelief</span></span>
<span class="r-in"><span class="va">opt_hparams</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>lr <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span>
<span class="r-in"><span class="va">optim</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/do.call.html" class="external-link">do.call</a></span><span class="op">(</span><span class="va">optim</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>params <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span><span class="op">)</span>, <span class="va">opt_hparams</span><span class="op">)</span><span class="op">)</span></span>
<span class="r-in"><span class="co"># run optimizer</span></span>
<span class="r-in"><span class="va">steps</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span class="r-in"><span class="va">x_steps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span></span>
<span class="r-in"><span class="va">y_steps</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span></span>
<span class="r-in"><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_len</a></span><span class="op">(</span><span class="va">steps</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span class="r-in">     <span class="va">x_steps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">y_steps</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">as.numeric</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">optim</span><span class="op">$</span><span class="fu">zero_grad</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">z</span> <span class="op">&lt;-</span> <span class="fu">test_matyas</span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">z</span><span class="op">$</span><span class="fu">backward</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in">     <span class="va">optim</span><span class="op">$</span><span class="fu">step</span><span class="op">(</span><span class="op">)</span></span>
<span class="r-in"><span class="op">}</span></span>
<span class="r-in"><span class="op">}</span></span>
<span class="r-in"></span>
</code></pre></div>
    </div>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Gilberto Camara, Rolf Simoes, Felipe Souza, Alber Sanchez.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.2.</p>
</div>

      </footer></div>

  


  

  </body></html>

